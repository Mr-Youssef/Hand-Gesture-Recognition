{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **All Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image, ImageFilter\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing data from google drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_drive():\n",
    "    gdown.download('https://drive.google.com/uc?id=1JLxhdIddq6_vKlHml7jT48VaeXoJjvpR', 'dataset.zip', quiet=False)\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(\"/content/dataset.zip\", 'r')\n",
    "    zip_ref.extractall(\"dataset\")\n",
    "    zip_ref.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing data from a folder on your computer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_folder(folder_path):\n",
    "    return folder_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure all images have the same extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_extension():\n",
    "    dataset_path = '/content/dataset'\n",
    "    # Loop over the women and men folders\n",
    "    for gender in ['Women', 'men']:\n",
    "        gender_path = os.path.join(dataset_path, gender)\n",
    "        for i in range(6):\n",
    "            folder_path = os.path.join(gender_path, str(i))\n",
    "            # Loop over the files in the folder\n",
    "            extensions = set()\n",
    "            for filename in os.listdir(folder_path):\n",
    "                if os.path.isfile(os.path.join(folder_path, filename)):\n",
    "                    file_extension = os.path.splitext(filename)[1]\n",
    "                    if file_extension not in ['.JPG']:\n",
    "                        print(f'Error: {filename} has an invalid extension ({file_extension})')\n",
    "                        os.remove(os.path.join(folder_path, filename))\n",
    "                    extensions.add(file_extension)\n",
    "            # Check that all files have the same extension\n",
    "            if (len(extensions) == 0):\n",
    "                print('All files have the same extension')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(folder_path):    \n",
    "  women_dataset = folder_path + '/Women'\n",
    "  men_dataset = folder_path +'/men'\n",
    "\n",
    "  images_paths = []\n",
    "  # dataset_labels = []\n",
    "  classes_sizes=[]\n",
    "  corrupted=0\n",
    "  corrupted_imgs=[]\n",
    "  for i in range(6):\n",
    "    women_folder_path = os.path.join(women_dataset, str(i))\n",
    "    men_folder_path = os.path.join(men_dataset, str(i))\n",
    "    # Get the file paths in the folders\n",
    "    women_files = [os.path.join(women_folder_path, f) for f in os.listdir(women_folder_path) if os.path.isfile(os.path.join(women_folder_path, f))]\n",
    "    men_files = [os.path.join(men_folder_path, f) for f in os.listdir(men_folder_path) if os.path.isfile(os.path.join(men_folder_path, f))]\n",
    "\n",
    "    imgs = women_files + men_files\n",
    "    for img in imgs:\n",
    "      if not (img.endswith(\".JPG\") or img.endswith(\".jpg\")) and not (img.endswith(\".png\") or img.endswith(\".PNG\")):         # in case of desktop.ini file or any other file\n",
    "          continue\n",
    "\n",
    "      try:\n",
    "          temp = Image.open(img)\n",
    "      except (IOError, SyntaxError) as e:   # in case of corrupted images\n",
    "          print('Bad file:', temp)\n",
    "          corrupted+=1\n",
    "          continue \n",
    "      path = img.split('/')\n",
    "      images_paths.append(img)\n",
    "      \n",
    "      \n",
    "    classes_sizes.append(len(imgs))    \n",
    "  print(\"Total number of images = \", len(images_paths))\n",
    "  print(\"Number of corrupted images  = \", corrupted)\n",
    "  print(\"Number of images per class = \", classes_sizes)\n",
    "  return images_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_get_labels(images_paths, input_type):\n",
    "  random.seed(7)\n",
    "  random.shuffle(images_paths)\n",
    "  dataset_labels = []\n",
    "  for img in images_paths:\n",
    "    if input_type == 'drive':\n",
    "      path=img.split('/')\n",
    "      dataset_labels.append(path[4])\n",
    "    elif input_type == 'folder':\n",
    "      path=img.split('/')[-1].split('\\\\')\n",
    "      dataset_labels.append(path[1])\n",
    "  return images_paths, dataset_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting dataset into train and validation and test  (70%, 10%, 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(images_paths, dataset_labels):\n",
    "\n",
    "    # splitting training into 80% and testing into 20%\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(images_paths, dataset_labels, test_size=0.2, random_state=42, stratify=dataset_labels)\n",
    "\n",
    "    # splitting training into 70% and validation into 10%\n",
    "    train_paths, validation_paths, train_labels, validation_labels = train_test_split(train_paths, train_labels, test_size=0.1, random_state=42, stratify=train_labels)\n",
    "\n",
    "    print(f\"Size of training data = {len(train_paths)}, Size of validation data = {len(validation_paths)}, Size of testing data = {len(test_paths)}\")\n",
    "\n",
    "    # check\n",
    "    total = len(train_paths) + len(validation_paths) + len(test_paths)\n",
    "    if total == len(images_paths):\n",
    "        print(\"Splitting is done correctly\")\n",
    "    else:  \n",
    "        print(\"Error in splitting\")\n",
    "    \n",
    "    return train_paths, train_labels, validation_paths, validation_labels, test_paths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(input_type, folder_path = '/content/dataset'):       #default folder path if importing from drive\n",
    "    if input_type == 'drive':\n",
    "        read_from_drive()\n",
    "        same_extension()\n",
    "    elif input_type == 'folder':\n",
    "        pass \n",
    "        \n",
    "    images_paths = gather_data(folder_path)\n",
    "    images_paths, dataset_labels = shuffle_and_get_labels(images_paths, input_type)\n",
    "    train_images, train_labels, validation_images, validation_labels, test_paths, test_labels = splitting(images_paths, dataset_labels)\n",
    "    \n",
    "    return train_images, train_labels, validation_images, validation_labels, test_paths, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_py():\n",
    "    !jupyter nbconvert --to script input_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook input_utils.ipynb to script\n",
      "[NbConvertApp] Writing 5512 bytes to input_utils.py\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_py()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
